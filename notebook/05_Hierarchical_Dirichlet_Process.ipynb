{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "from gensim.models import HdpModel\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df4 = pd.read_csv('../data/review_gopay_newest_sort.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords if not already downloaded\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "custom_stopwords = {'yang', 'aja', 'yg', 'nya', 'sih', 'oh', 'e', 'deh', 'ya', 'kan', 'nih'}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "# remove word from stopwords\n",
    "stop_words = stop_words - {'tidak'} \n",
    "\n",
    "# define synonym dictionary\n",
    "synonym_dict = {\n",
    "    'apk': 'aplikasi', 'app': 'aplikasi', \n",
    "    'oke': 'ok', 'gak': 'tidak', 'ga': 'tidak', 'gk': 'tidak',\n",
    "    'g':'tidak','tf': 'transfer'\n",
    "}\n",
    "\n",
    "# function to normalize elongated words (e.g., sukaaaa -> suka)\n",
    "def normalize_repeated_chars(word):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', word)                            # replace >=3 chars with 1 char\n",
    "\n",
    "# Preprocess function with normalization\n",
    "def tokenize_text(text):\n",
    "    if isinstance(text, str):                                           # check if the input is a string\n",
    "        text = text.lower()                                             # convert to lowercase\n",
    "        text = re.sub(r'\\d+', '__num__', text)                                 # remove numbers\n",
    "        text = re.sub(r'\\W+', ' ', text)                                # remove punctuation\n",
    "        text = re.sub(r'http\\S+|www\\S+', '__url__', text)                      # remove URLs\n",
    "        tokens = text.split()                                           # split into tokens\n",
    "        tokens = [normalize_repeated_chars(word) for word in tokens]    # normalize elongated words\n",
    "        tokens = [synonym_dict.get(word, word) for word in tokens]      # replace with synonym if it exists\n",
    "        tokens = [word for word in tokens if word not in stop_words]    # remove stopwords\n",
    "        return tokens\n",
    "    return []                                                           # if not string, return an empty list\n",
    "\n",
    "# Apply preprocessing\n",
    "df4['tokens'] = df4['content'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output\n",
    "pd.options.display.max_colwidth = None\n",
    "df4[['content', 'tokens']].sample(n=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary and corpus for HDP\n",
    "dictionary = corpora.Dictionary(df4['tokens'])\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df4['tokens'] if tokens]  # Filter out empty lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the dictionary (word to ID mapping)\n",
    "print(f'The content of dictionary: \\n{list(dictionary.token2id.items())[:5]}\\n')\n",
    "\n",
    "# print top documents in the corpus (+ freq)\n",
    "print('The content of corpus:')\n",
    "for i in range(5):\n",
    "    doc = corpus[i]\n",
    "    decoded_doc = [(dictionary[id], freq) for id, freq in doc]\n",
    "    print(f\"Document {i+1}: {decoded_doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the HDP model\n",
    "hdp_model = HdpModel(corpus=corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also explore the coherence score (if desired)\n",
    "coherence_model_hdp = CoherenceModel(model=hdp_model, texts=df4['tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_score_hdp = coherence_model_hdp.get_coherence()\n",
    "print(f'HDP Coherence Score: {coherence_score_hdp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics\n",
    "print(\"Topics found by HDP:\")\n",
    "for idx, topic in hdp_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign topics to documents\n",
    "df4['topic'] = -1  # Initialize with -1 for unclassifiable topics\n",
    "for i, item in enumerate(corpus):\n",
    "    if item:  # Only assign a topic if the item is not empty\n",
    "        topic_info = hdp_model.get_document_topics(item)\n",
    "        if topic_info:\n",
    "            df4.at[i, 'topic'] = max(topic_info, key=lambda x: x[1])[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
